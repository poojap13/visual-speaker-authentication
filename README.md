##  `README.md` â€” *Enhancing Visual Speaker Authentication Using Dynamic Lip Movement and Meta-Learning*  
ğŸ—“ï¸ Last Updated: 2025-04-14


---

### ğŸ¯ Project Overview

This project proposes a **Visual Speaker Authentication (VSA)** system that leverages **dynamic lip movements** for secure and spoof-resistant identity verification. It addresses challenges in conventional speaker authentication systems such as:
- High registration effort
- Susceptibility to DeepFake attacks
- Poor generalization to unseen users

We adopt a **few-shot meta-learning approach (MAML)** using optical flow inputs, enabling:
- **Fast adaptation to new users with minimal samples**
- **Strong spoof detection against AI-generated fakes**
- **Evaluation on speaker-disjoint splits**

---

### â“ Research Questions

**RQ1:** Can few-shot learning (MAML) reduce speaker registration requirements compared to traditional CNN-based models?

**RQ2:** Are dynamically extracted visual features (optical flow of lip movements) effective for detecting DeepFake spoofing?

---

### ğŸ§ª Dataset

- ğŸ“¦ **GRID Corpus**: 34 speakers, frontal face videos with matching audio
- ğŸ­ **Fake Generation**: Created using [Wav2Lip](https://github.com/Rudrabha/Wav2Lip), mixing real faces with mismatched audio to produce DeepFakes
- ğŸŒ€ **Optical Flow Extraction**: Lip regions are isolated using Dlib and processed with Farneback's method to capture temporal motion

---

### ğŸ—ƒï¸ Project Structure

```
visual-speaker-authentication/
â”œâ”€â”€ data/                       # Dataset instructions
â”œâ”€â”€ notebooks/                 # Jupyter notebooks (preprocessing, training, eval)
â”œâ”€â”€ results/                   # Metrics, confusion matrix, ROC curves
â”œâ”€â”€ saved_models/              # Trained model checkpoints (.pth)
â”œâ”€â”€ src/                       # Modular Python scripts
â”œâ”€â”€ main.py                    # End-to-end pipeline script
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ .gitignore                 # Ignore configs, models, cache
â””â”€â”€ README.md                  # This documentation
```

---

### ğŸ“” Notebooks

| File                                       | Purpose                                                                 |
|--------------------------------------------|-------------------------------------------------------------------------|
| `Fakedataset_generation.ipynb`             | Generate DeepFake videos using Wav2Lip                                 |
| `OpticalFlowFakeSpeakers.ipynb`            | Extract lip optical flow features                                      |
| `VSA_FINALMAML.ipynb`                      | Full MAML training and testing                                         |
| `VSA_hyperparametertunning&20tasks.ipynb`  | Optuna tuning + reduced-task generalization setup                     |

---

### ğŸ§  Core Scripts (in `src/`)

| Script                 | Description                                                             |
|------------------------|-------------------------------------------------------------------------|
| `generate_fakes.py`    | Batch generates fake videos using Wav2Lip for all speakers              |
| `extract_optical_flow.py` | Extracts lip optical flow from real/fake videos (saves `.npy` files) |
| `train_maml.py`        | Final MAML model training with Optuna-best hyperparameters              |
| `tune_optuna.py`       | Hyperparameter tuning (dropout, inner_lr, meta_lr, shots) using Optuna |

---

### ğŸ”§ How to Run

#### ğŸ“¦ 1. Clone and Install
```bash
git clone https://github.com/poojap13/visual-speaker-authentication.git
cd visual-speaker-authentication
pip install -r requirements.txt
```

#### ğŸ“¥ 2. Download GRID Dataset
```bash
cd data
# See: DownloadGridDataset.ipynb or README.md for script
```

#### ğŸ§ª 3. Generate Fake Videos
```bash
python src/generate_fakes.py
```

#### ğŸï¸ 4. Extract Optical Flow
```bash
python src/extract_optical_flow.py
```

#### ğŸ§  5. Tune Hyperparameters (Optional)
```bash
python src/tune_optuna.py
```

#### ğŸ§¬ 6. Train Final MAML Model
```bash
python src/train_maml.py
```

#### âœ… 7. Full Pipeline
```bash
python main.py
```

---

### ğŸ§¾ Results Summary (on speaker-disjoint setup)

| Model           | Accuracy | AUC   | EER   | HTER  | F1    |
|----------------|----------|-------|-------|-------|-------|
| MAML (Final)   | 99.67%   | 0.997 | 0.000 | 0.003 | 0.996 |

ğŸ“ See: `results/` folder for confusion matrix, ROC curves, and logs

---

### ğŸ§¬ Final Hyperparameters (Tuned via Optuna)

```yaml
dropout:       0.3766
inner_lr:      0.0204
meta_lr:       0.0006
shots:         3
epochs:        15
tasks/epoch:   50
```

---

### ğŸ“Œ Reproducibility Checklist (v2.0 âœ…)

- âœ… Dataset statistics, splits, preprocessing steps
- âœ… Hyperparameter tuning details (via Optuna)
- âœ… All training & evaluation code provided
- âœ… Pre-trained model: `saved_models/best_model3.pth`
- âœ… Confusion matrix, ROC, metrics (`results/`)
- âœ… Training logs and curves included

---

### ğŸ”— Links

- ğŸ“„ Overleaf Report: [Overleaf Final Report](https://www.overleaf.com/project/67e76f407a248c43d6fd131d)
- ğŸ“¦ Wav2Lip GitHub: [https://github.com/Rudrabha/Wav2Lip](https://github.com/Rudrabha/Wav2Lip)
- ğŸ—ƒ GRID Corpus: [https://spandh.dcs.shef.ac.uk/gridcorpus/](https://spandh.dcs.shef.ac.uk/gridcorpus/)
- ğŸ”— GitHub Repo: [https://github.com/poojap13/visual-speaker-authentication](https://github.com/poojap13/visual-speaker-authentication)

---

### ğŸ‘©â€ğŸ’» Author

**Pooja Pathare**  
MSc in Computer Science, Lakehead University  
Supervisor: Dr. Garima Bajwa

---

### ğŸ“œ License

This project is intended for **academic research and non-commercial use only**.  
Please cite the original repositories (Wav2Lip, Learn2Learn, GRID Corpus) if used.

